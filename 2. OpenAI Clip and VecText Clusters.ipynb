{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af07aa94",
   "metadata": {},
   "source": [
    "# 2. Visual Search - OpenAI Clip and VecText Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8608749",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/retkowsky/images/blob/master/visualsearchlogo.jpg?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b881c5c",
   "metadata": {},
   "source": [
    "# Visual Search with Azure Cognitive Search, Sentence Transformers, Azure Computer Vision and bar code/QR code detection\n",
    "\n",
    "## Description\n",
    "The goal of this is **Azure AI asset is to enable search over Text and Images using Azure Cognitive Search**. The technique was inspired by a research article which show how to **convert vectors (embeddings) to text which allows the Cognitive Search service to leverage the inverted index to quickly find the most relevant items**. For this reason, any model that will convert an object to a vector can be leveraged if the number of dimensions in the resulting vector is less than 3,000. It also allows users to leverage existing pretrained or fine-tuned models.<br><br>\n",
    "\n",
    "This technique has shown to be incredibly effective and easy to implement. We are using **Sentence Transformers, which is an OpenAI clip model wrapper**. We need to embed all our existing catalog of images. Then the objects embedding are converted into a set of fake terms and all the results are stored into an Azure Cognitive Search index for handling all the search requests.\n",
    "For example, if an embedding looked like [-0,21, .123, ..., .876], this might be converted to a set of fake terms such as: “A1 B3 … FED0”. This is what is sent as the search query to Azure Cognitive Search.<br><br>\n",
    "\n",
    "We can **enrich the Azure Cognitive Search index by using extracted text from the images using Azure Read API**. We can also detect and extract any information from **bar code and/or QR code** that might be available in the products catalog images. And we can use also **Azure Computer Vision as well to detect the dominant colors of the image, the tags that can describe the image and the caption of each image**. All these information will be ingested into the Azure Cognitive Search index.<br><br>\n",
    "\n",
    "The goal of this asset is to be able to use the inverted index within Azure Cognitive Search to be able to quickly find vectors stored in the search index that are like a vector provided as part of a search query and/or using any AI extracted information (text, dominant colors, …). Unlike techniques like cosine similarity which are slow to process large numbers of items, this leverages an inverted index which enables much more data to be indexed and searched.<br>\n",
    "\n",
    "## Process\n",
    "\n",
    "- We have here a collection of catalog images (466 images).\n",
    "- For each of these images, we will embed them using Sentence Transformers.  Sentence Transformer can be used to map images and texts to the same vector space. As model, we use the OpenAI CLIP Model which was trained on a large set of images and image alt texts.\n",
    "- We can retrieve any text from these images using Azure Read API (if any text is available)\n",
    "- We can retrieve any text information from any bar code or QR code (if any)\n",
    "- All these information will be ingested into an Azure Cognitive Search index\n",
    "- Then if you have a field image, you can embed it and extract any text/barcode information and call the Azure Cognitive Search index to retrieve any similar images using vecText similarity and/or using any query text from the extracted text\n",
    "\n",
    "\n",
    "<img src=\"https://github.com/retkowsky/images/blob/master/process.png?raw=true\">\n",
    "\n",
    "Field images are available in the field images directory (number of images=53)\n",
    "\n",
    "\n",
    "## Azure products documentation\n",
    "- https://azure.microsoft.com/en-us/products/search/ \n",
    "- https://azure.microsoft.com/en-us/products/cognitive-services/computer-vision/#overview \n",
    "- https://learn.microsoft.com/en-us/azure/cognitive-services/Computer-vision/how-to/call-read-api \n",
    "- https://zbar.sourceforge.net/ \n",
    "- https://github.com/liamca/vector-search\n",
    "\n",
    "## Research article\n",
    "https://www.researchgate.net/publication/305910626_Large_Scale_Indexing_and_Searching_Deep_Convolutional_Neural_Network_Features\n",
    "    \n",
    "## Directories\n",
    "- **images**: We have two directories (catalog images, field images)\n",
    "- **model**: Directory to save the clusters of the model\n",
    "- **results**: Directory to save some results\n",
    "- **test**: Directory that contains some testing images\n",
    "\n",
    "## Python notebooks\n",
    "\n",
    "### 0. Settings.ipynb\n",
    "Notebook that contains the link to the images and the importation process of the python required libraries\n",
    "\n",
    "### 1. Catalog images exploration.ipynb\n",
    "This notebook will display some catalog and field images\n",
    "\n",
    "### 2. OpenAI Clip and VecText Clusters.ipynb\n",
    "This notebook will explain what sentence transformers is and will generate the clusters\n",
    "This notebook analyzes a set of existing images to determine a set of \"cluster centers\" that will be used to determine which \"fake words\" are generated for a vector\n",
    "This notebook will take a test set of files (testSamplesToTest) and determine the optimal way to cluster vectors into fake words that will be indexed into Azure Cognitive Search\n",
    "\n",
    "### 3. VecText generation.ipynb\n",
    "This notebook will generate the vectext embedding for all the catalog images\n",
    "\n",
    "### 4. BarCode Information extraction.ipynb\n",
    "This notebook will detect any barcode or QR code from the catalog images and will extract the information\n",
    "\n",
    "### 5. Azure CV for OCR, tags, colors and captions.ipynb\n",
    "This notebook will use Azure Computer Vision or OCR, colors, tags and caption extraction for each of the catalog images.\n",
    "\n",
    "### 6. Azure Cognitive Search Index Generation.ipynb\n",
    "This notebook will show how to ingest all the information into an Azure Cognitive Search index.\n",
    "\n",
    "### 7. Calling Azure Cognitive Search.ipynb\n",
    "We can now test the index using some images similarity visual search or free text queries using azure Cognitive Search.\n",
    "\n",
    "## Python files\n",
    "\n",
    "- **azureCognitiveSearch.py**\n",
    "This python file contains many functions to manage and use Azure Cognitive Search\n",
    "\n",
    "- **myfunctions.py**\n",
    "This python file contains many generic functions used in all the notebooks\n",
    "\n",
    "- **vec2Text.py**\n",
    "This python file contains some functions for the sentence transformers model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff289dd0",
   "metadata": {},
   "source": [
    "24-oct-2022 Serge Retkowsky | serge.retkowsky@microsoft.com | https://www.linkedin.com/in/serger/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88db6393",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import concurrent.futures\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from IPython.display import Image as IPDImage\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import myfunctions as my\n",
    "import vec2Text\n",
    "\n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "import azureml.core\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad83c476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-10-2022 14:28:22\n"
     ]
    }
   ],
   "source": [
    "print(my.get_today())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4109ab6f",
   "metadata": {},
   "source": [
    "## 1. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4616432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'azureservices.py'\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "subscription_id = config.get('AzureML', 'subscription_id')\n",
    "resource_group = config.get('AzureML', 'resource_group')\n",
    "workspace_name = config.get('AzureML', 'workspace_name')\n",
    "                            \n",
    "ws = Workspace(subscription_id, resource_group, workspace_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef18ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "experiment = Experiment(workspace=ws, name=\"VisualSearch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35073ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Name</th><th>Workspace</th><th>Report Page</th><th>Docs Page</th></tr><tr><td>VisualSearch</td><td>azuremlvision</td><td><a href=\"https://ml.azure.com/experiments/id/4048626f-d7f2-47b4-aacb-37277739794a?wsid=/subscriptions/70b8f39e-8863-49f7-b6ba-34a80799550c/resourcegroups/azuremlvision-rg/workspaces/azuremlvision&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.Experiment?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Experiment(Name: VisualSearch,\n",
       "Workspace: azuremlvision)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbc4ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_center_file = 'cluster_centers_images.pkl'\n",
    "testSamplesToTest = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa821eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image are available here: ./images/catalog_images\n"
     ]
    }
   ],
   "source": [
    "IMAGES_DIR = \"./images/catalog_images\"\n",
    "print(\"Image are available here:\", IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8ada50",
   "metadata": {},
   "source": [
    "## 2. Open AI with sentence transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b5483",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/openai/CLIP/raw/main/CLIP.png\">\n",
    "\n",
    "Here we will use sentence transformer which is an OpenAI clip wrapper<br>\n",
    "https://github.com/UKPLab/sentence-transformers\n",
    "<br><br>\n",
    "\n",
    "- Blog: https://openai.com/blog/clip/\n",
    "- Model Card: https://github.com/openai/CLIP/blob/main/model-card.md\n",
    "- Paper: https://arxiv.org/abs/2103.00020\n",
    "\n",
    "List of models: https://www.sbert.net/docs/pretrained_models.html#image-text-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce78681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OpenAI Clip model: clip-ViT-B-32\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "model = vec2Text.openai_clip_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca4382",
   "metadata": {},
   "source": [
    "## 3. VecText Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77df5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images Catalog files: 466\n"
     ]
    }
   ],
   "source": [
    "files = vec2Text.get_files_in_dir(IMAGES_DIR)\n",
    "print('Total images Catalog files:', len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9e76b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Dimensions: 512\n"
     ]
    }
   ],
   "source": [
    "# Look at a single document and determine the number\n",
    "# of dimensions the resulting vectors will have\n",
    "dimensions = vec2Text.calculate_dimensions(files[0], model=model)\n",
    "print('Vector Dimensions:', dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "354b8def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-10-2022 14:28:26 Adding test vectors to the dictionnary...\n",
      "Test Samples: 100 \n",
      "\n",
      "Processed: 10 of 100\n",
      "Processed: 20 of 100\n",
      "Processed: 30 of 100\n",
      "Processed: 40 of 100\n",
      "Processed: 50 of 100\n",
      "Processed: 60 of 100\n",
      "Processed: 70 of 100\n",
      "Processed: 80 of 100\n",
      "Processed: 90 of 100\n",
      "Processed: 100 of 100\n",
      "\n",
      "Done in 14 seconds\n"
     ]
    }
   ],
   "source": [
    "start = my.now()\n",
    "vecDict = []\n",
    "\n",
    "print(my.get_today(), \"Adding test vectors to the dictionnary...\")\n",
    "print(\"Test Samples:\", testSamplesToTest, \"\\n\")\n",
    "\n",
    "vecDict = vec2Text.initialize_vector_dictionary(dimensions)\n",
    "idx = 0\n",
    "\n",
    "for file in files:\n",
    "    # Embedding the samples\n",
    "    cur_vec = vec2Text.image_embedding(file, model=model)\n",
    "\n",
    "    for d in range(dimensions):\n",
    "        vecDict[str(d)].append(cur_vec[d])\n",
    "\n",
    "    idx += 1\n",
    "\n",
    "    if idx % 10 == 0:\n",
    "        print('Processed:', idx, 'of', testSamplesToTest)\n",
    "\n",
    "    if idx == testSamplesToTest:\n",
    "        break\n",
    "\n",
    "print(\"\\nDone in\", (my.now() - start).in_words(locale='en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd776eb7",
   "metadata": {},
   "source": [
    "### Running the k-means algorithm using the optimal k values and find the cluster centers for the 512 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03928f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24-10-2022 14:28:40 Finding the cluster centers...\n",
      "\n",
      "Processed 10 of 512\n",
      "Processed 20 of 512\n",
      "Processed 30 of 512\n",
      "Processed 40 of 512\n",
      "Processed 50 of 512\n",
      "Processed 60 of 512\n",
      "Processed 70 of 512\n",
      "Processed 80 of 512\n",
      "Processed 90 of 512\n",
      "Processed 100 of 512\n",
      "Processed 110 of 512\n",
      "Processed 120 of 512\n",
      "Processed 130 of 512\n",
      "Processed 140 of 512\n",
      "Processed 150 of 512\n",
      "Processed 160 of 512\n",
      "Processed 170 of 512\n",
      "Processed 180 of 512\n",
      "Processed 190 of 512\n",
      "Processed 200 of 512\n",
      "Processed 210 of 512\n",
      "Processed 220 of 512\n",
      "Processed 230 of 512\n",
      "Processed 240 of 512\n",
      "Processed 250 of 512\n",
      "Processed 260 of 512\n",
      "Processed 270 of 512\n",
      "Processed 280 of 512\n",
      "Processed 290 of 512\n",
      "Processed 300 of 512\n",
      "Processed 310 of 512\n",
      "Processed 320 of 512\n",
      "Processed 330 of 512\n",
      "Processed 340 of 512\n",
      "Processed 350 of 512\n",
      "Processed 360 of 512\n",
      "Processed 370 of 512\n",
      "Processed 380 of 512\n",
      "Processed 390 of 512\n",
      "Processed 400 of 512\n",
      "Processed 410 of 512\n",
      "Processed 420 of 512\n",
      "Processed 430 of 512\n",
      "Processed 440 of 512\n",
      "Processed 450 of 512\n",
      "Processed 460 of 512\n",
      "Processed 470 of 512\n",
      "Processed 480 of 512\n",
      "Processed 490 of 512\n",
      "Processed 500 of 512\n",
      "Processed 510 of 512\n",
      "\n",
      "Done in 8 minutes 15 seconds\n"
     ]
    }
   ],
   "source": [
    "start = my.now()\n",
    "\n",
    "print(my.get_today(), \"Finding the cluster centers...\\n\")\n",
    "clusterCenters = vec2Text.find_cluster_centers(dimensions, vecDict)\n",
    "print(\"\\nDone in\", (my.now() - start).in_words(locale='en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba36f9b",
   "metadata": {},
   "source": [
    "## 4. Saving clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5da4c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Directory: model has been created\n"
     ]
    }
   ],
   "source": [
    "PKL_FOLDER = \"model\"\n",
    "my.create_dir(PKL_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d726ae31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving cluster centers into: cluster_centers_images.pkl\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving cluster centers into:\", cluster_center_file)\n",
    "\n",
    "os.chdir(PKL_FOLDER)\n",
    "with open(cluster_center_file, 'wb') as pickle_out:\n",
    "    pickle.dump(clusterCenters, pickle_out)\n",
    "os.chdir(\"..\")\n",
    "\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6896f0",
   "metadata": {},
   "source": [
    "### Saving to Azure ML datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36c60e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating arguments.\n",
      "Arguments validated.\n",
      "Uploading file to model\n",
      "Uploading an estimated of 1 files\n",
      "Uploading model/cluster_centers_images.pkl\n",
      "Uploaded model/cluster_centers_images.pkl, 1 files out of an estimated total of 1\n",
      "Uploaded 1 files\n",
      "Creating new dataset\n"
     ]
    }
   ],
   "source": [
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "datastore = workspace.get_default_datastore()\n",
    "\n",
    "ds = Dataset.File.upload_directory(src_dir=PKL_FOLDER,\n",
    "                                   target=DataPath(datastore, PKL_FOLDER),\n",
    "                                   show_progress=True,\n",
    "                                   overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4838d9",
   "metadata": {},
   "source": [
    "## 5. Clusters file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb55a5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in directory: model \n",
      "\n",
      "1 \t 2022-10-24 14:36:57.169631 29.1 kB \t cluster_centers_images.pkl\n"
     ]
    }
   ],
   "source": [
    "my.list_dir(PKL_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7dcbdc",
   "metadata": {},
   "source": [
    "> End of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47820f21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
